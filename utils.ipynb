{"cells":[{"cell_type":"code","execution_count":null,"id":"a2eefccd","metadata":{"_cell_guid":"337ffaea-34f5-4b97-bdac-2f4ae100c007","_uuid":"a185934d-d74c-4efe-b0be-a2dfb033cef2","execution":{"iopub.execute_input":"2023-11-10T02:03:06.777416Z","iopub.status.busy":"2023-11-10T02:03:06.776312Z","iopub.status.idle":"2023-11-10T02:03:12.327158Z","shell.execute_reply":"2023-11-10T02:03:12.325676Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":5.559671,"end_time":"2023-11-10T02:03:12.330536","exception":false,"start_time":"2023-11-10T02:03:06.770865","status":"completed"},"tags":[],"id":"a2eefccd"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from torchvision.ops import DropBlock2d\n","from torchvision.ops import stochastic_depth\n","from torchvision.datasets import FashionMNIST\n","\n","\n","# from torchvision.transforms import v2\n","\n","#==============================================================================================================================================================\n","# RESNET ARCHITECTURE CLASS\n","#==============================================================================================================================================================\n","\n","\n","# Define a BasicBlock for the ResNet-like architecture\n","class BasicBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, stride1=2):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.downsample = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False),\n","            nn.BatchNorm2d(out_channels)\n","            ) if (stride != 1 or in_channels != out_channels) and (out_channels == 128 or out_channels == 256 or out_channels ==512) else nn.Identity()\n","\n","\n","    def forward(self, x):\n","        residual = self.downsample(x)\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x += residual\n","        x = self.relu(x)\n","        return x\n","\n","\n","# Define the Deformable Convolution Block\n","class DeformConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DeformConvBlock, self).__init__()\n","        self.offset_conv = nn.Conv2d(in_channels, 18, kernel_size=3, padding=1)  # 18 channels for x, y offsets, and masks\n","        self.deform_conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        offset = self.offset_conv(x)\n","\n","        # Adjust the mask channels to be half of the offset channels\n","        mask = offset[:, :offset.size(1) // 2, ...]\n","\n","        x = torchvision.ops.deform_conv2d(input=x,\n","                                          offset=offset,\n","                                          weight=self.deform_conv.weight,\n","                                          bias=None,  # Set bias to None if you don't want bias\n","                                          padding=self.deform_conv.padding,\n","                                          mask=mask,  # Use the adjusted mask\n","                                          stride=self.deform_conv.stride,\n","                                          dilation=self.deform_conv.dilation)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","\n","# Define a BasicBlock for the ResNet-like architecture\n","class StoDepthBasicBlock(BasicBlock):\n","    def __init__(self, in_channels, out_channels, stride=1, stride1=2, p=0.5, mode=\"batch\"):\n","        super().__init__(in_channels, out_channels, stride, stride1)\n","        self.p = p\n","        self.mode = mode\n","\n","    def forward(self, x):\n","        residual = self.downsample(x)\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = stochastic_depth(input=x,p=self.p,mode=self.mode)\n","        x += residual\n","        x = self.relu(x)\n","        return x\n","\n","# Define a BasicBlock with DropBlock the ResNet-like architecture\n","class DropBlockBasicBlock(BasicBlock):\n","    def __init__(self, in_channels, out_channels, stride=1, stride1=2, p=0.1, block_size=1):\n","        super().__init__(in_channels, out_channels, stride, stride1)\n","        self.dropblock = DropBlock2d(p, block_size)\n","\n","    def forward(self, x):\n","        residual = self.downsample(x)\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.dropblock(x)\n","        x += residual\n","        x = self.relu(x)\n","        return x\n","\n","# Define the ResNet18 architecture\n","class ResNet18(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ResNet18, self).__init__()\n","\n","        self.model = models.resnet18(pretrained=True)\n","        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.model.layer4 = nn.Identity()\n","        self.model.fc = nn.Linear(256, num_classes)\n","\n","    def make_layer(self, in_channels, out_channels, blocks, stride1, stride=1):\n","        layers = []\n","        layers.append(BasicBlock(in_channels, out_channels, stride, stride1))\n","        for _ in range(1, blocks):\n","            layers.append(BasicBlock(out_channels, out_channels, stride, stride1=1))\n","        return nn.Sequential(*layers)\n","\n","\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Define the CustomResNet architecture that uses DeformConvBlock and Resnet18\n","class CustomResNet(nn.Module):\n","    def __init__(self, num_classes, layers_to_deform):\n","        super(CustomResNet, self).__init__()\n","\n","        self.model = models.resnet18(pretrained=True)\n","        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","        if 1 in layers_to_deform:\n","            self.model.layer1 = self.make_layer_with_deform(64, 64, blocks=2, stride=1, stride1=1)\n","        if 2 in layers_to_deform:\n","            self.model.layer2 = self.make_layer_with_deform(64, 128, blocks=2, stride=1, stride1=2)\n","        if 3 in layers_to_deform:\n","            self.model.layer3 = self.make_layer_with_deform(128, 256, blocks=2, stride=1, stride1=2)\n","        self.model.layer4 = nn.Identity()\n","        self.model.fc = nn.Linear(256, num_classes)\n","\n","    def make_layer(self, in_channels, out_channels, blocks, stride1, stride=1):\n","        layers = []\n","        layers.append(BasicBlock(in_channels, out_channels, stride, stride1))\n","        for _ in range(1, blocks):\n","            layers.append(BasicBlock(out_channels, out_channels, stride, stride1=1))\n","        return nn.Sequential(*layers)\n","\n","    def make_layer_with_deform(self, in_channels, out_channels, blocks, stride, stride1):\n","        layers = []\n","        layers.append(BasicBlock(in_channels, out_channels, stride, stride1))\n","        layers.append(DeformConvBlock(out_channels, out_channels))  # Add a deformable convolution block\n","        for _ in range(1, blocks - 1):\n","            layers.append(BasicBlock(out_channels, out_channels))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Define the CustomResNet architecture that uses Stochastic Depth\n","class CustomResNetStoDepth(CustomResNet):\n","    def __init__(self, num_classes, p, mode):\n","        super(CustomResNetStoDepth, self).__init__(num_classes, [])\n","        self.p = p\n","        self.mode = mode\n","        self.model = models.resnet18(pretrained=True)\n","        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.model.layer1 = self.make_layer_stodepth(64, 64, blocks=2, stride1=1)\n","        self.model.layer2 = self.make_layer_stodepth(64, 128, blocks=2, stride1=2)\n","        self.model.layer3 = self.make_layer_stodepth(128, 256, blocks=2, stride1=2)\n","        self.model.layer4 = self.make_layer_stodepth(256, 512, blocks=2, stride1=2)\n","        self.model.fc = nn.Linear(512, num_classes)\n","\n","    def make_layer_stodepth(self, in_channels, out_channels, blocks, stride1, stride=1):\n","        layers = []\n","        layers.append(StoDepthBasicBlock(in_channels, out_channels, stride, stride1, p=self.p, mode=self.mode))\n","        for _ in range(1, blocks):\n","            layers.append(StoDepthBasicBlock(out_channels, out_channels, stride, stride1=1, p=self.p, mode=self.mode))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Define CustomResNet architecture that uses DropBlock\n","class CustomResNetDropBlock(CustomResNet):\n","    def __init__(self, num_classes, p, block_size):\n","        super(CustomResNetDropBlock, self).__init__(num_classes, [])\n","        self.p = p\n","        self.block_size = block_size\n","        self.model = models.resnet18(pretrained=True)\n","        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.model.layer1 = self.make_layer_dropblock(64, 64, blocks=2, stride1=1)\n","        self.model.layer2 = self.make_layer_dropblock(64, 128, blocks=2, stride1=2)\n","        self.model.layer3 = self.make_layer_dropblock(128, 256, blocks=2, stride1=2)\n","        self.model.layer4 = self.make_layer_dropblock(256, 512, blocks=2, stride1=2)\n","        self.model.fc = nn.Linear(512, num_classes)\n","\n","    def make_layer_dropblock(self, in_channels, out_channels, blocks, stride1, stride=1):\n","        layers = []\n","        layers.append(DropBlockBasicBlock(in_channels, out_channels, stride, stride1, p=self.p, block_size=self.block_size))\n","        for _ in range(1, blocks):\n","            layers.append(DropBlockBasicBlock(out_channels, out_channels, stride, stride1=1, p=self.p, block_size=self.block_size))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","\n","\n","\n","\n","\n","\n","#==============================================================================================================================================================\n","# DATALOADING & TRAINING FUNCTION\n","#==============================================================================================================================================================\n","\n","\n","# Return train,validation and test dataloader\n","def fashionmnist_dataloader(batch_size, basic_aug = True):\n","    # PyTorch FashionMNIST\n","    fashion_mnist = FashionMNIST(download=True, train=True, root=\".\").train_data.float()\n","\n","    # Normal normalization\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224),antialias=True),\n","        transforms.ToTensor(),\n","        transforms.Normalize((fashion_mnist.mean()/255,), (fashion_mnist.std()/255,)),\n","\n","    ])\n","\n","    # if Basic Augmentation true/false for different dataset\n","    if basic_aug == True:\n","        # For basic technique augmentation\n","        basic_aug_transform = transforms.Compose([\n","            transforms.RandomResizedCrop(size=(224, 224), antialias=True),\n","            transforms.RandomHorizontalFlip(p=0.5),\n","            transforms.RandomVerticalFlip(p=0.5),\n","            transforms.ToTensor(),\n","            transforms.Normalize((fashion_mnist.mean()/255,), (fashion_mnist.std()/255,)),\n","\n","            ])\n","        train_dataset = FashionMNIST(root='./data', train=True, transform=basic_aug_transform, download=True)\n","    else:\n","        train_dataset = FashionMNIST(root='./data', train=True, transform=transform, download=True)\n","\n","\n","    test_dataset = FashionMNIST(root='./data', train=False, transform=transform, download=True)\n","\n","\n","    # Define  Dataloader\n","    train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2, random_state=42)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader\n","\n","\n","\n","# MixUp technique for Data Augmentation\n","def mixup(x, y, alpha):\n","\n","    lam = torch.distributions.beta.Beta(alpha, alpha).sample()\n","    index = torch.randperm(x.size(0))\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    mixed_y = lam * y + (1 - lam) * y[index]\n","\n","    mixed_y = mixed_y.long()\n","\n","    return mixed_x, mixed_y\n","\n","\n","\n","\n","# CutMix technique for Data Augmentation\n","def cutmix(images, labels, alpha):\n","    batch_size, channels, height, width = images.shape\n","\n","    # Initialize arrays to store mixed images and mixed labels\n","    mixed_images = torch.empty_like(images)\n","    mixed_labels = torch.empty_like(labels)\n","\n","    for i in range(batch_size):\n","        # Randomly choose another image from the batch\n","        j = torch.randint(0, batch_size, (1,)).item()\n","        image1, label1 = images[i], labels[i]\n","        image2, label2 = images[j], labels[j]\n","\n","        # Generate random lambda value from beta distribution\n","        lam = torch.distributions.beta.Beta(alpha, alpha).sample()\n","\n","        # Compute the cutmix image\n","        max_cut_width = int(width * 1)\n","        cut_width = int(max_cut_width * (1 - lam))\n","        x1 = torch.randint(0, width - cut_width, (1,)).item()\n","        y1 = torch.randint(0, height, (1,)).item()\n","        x2 = x1 + cut_width\n","        y2 = y1 + cut_width\n","\n","        mixed_image = image1.clone()\n","        mixed_image[:, y1:y2, x1:x2] = image2[:, y1:y2, x1:x2]\n","\n","        count_label1 = torch.sum(mixed_image == image1).item()\n","        count_label2 = torch.sum(mixed_image == image2).item()\n","\n","        if count_label1 >= count_label2:\n","            mixed_label = label1\n","        else:\n","            mixed_label = label2\n","\n","        mixed_label = mixed_label.long()\n","        mixed_images[i] = mixed_image\n","        mixed_labels[i] = mixed_label\n","\n","    return mixed_images, mixed_labels\n","\n","\n","#----------------------------------------------------------------------------------------------------------------------------------\n","\n","\n","# Normal train function\n","def train(model, criterion, train_loader, optimizer, epoch, device):\n","    model.train()\n","\n","    #for loss and accuracy tracking\n","    total_loss = 0.0\n","    total_correct = 0\n","    num_data = 0\n","\n","    for data, target in train_loader:\n","        data=data.to(device)\n","        target=target.to(device)\n","\n","        model.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        pred = torch.max(output, 1)[1]\n","        correct = (pred == target).sum().item()\n","        total_correct += correct\n","        num_data += len(data)\n","\n","    #calculate loss and accuracy for one epoch\n","    average_loss = total_loss / len(train_loader)\n","    accuracy = 100. * total_correct / num_data\n","\n","    print(f'Train Epoch: {epoch}\\tAverage Loss: {average_loss:.6f}\\tAccuracy: {accuracy:.2f}%')\n","\n","    return accuracy, average_loss\n","\n","\n","# MixUp train function\n","def train_mixup(model, criterion, train_loader, optimizer, epoch, alpha, device):\n","    model.train()\n","\n","    #for loss and accuracy tracking\n","    total_loss = 0.0\n","    total_correct = 0\n","    num_data = 0\n","\n","    for data, target in train_loader:\n","        data, target = mixup(data, target, alpha)\n","        data=data.to(device)\n","        target=target.to(device)\n","        model.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        pred = torch.max(output, 1)[1]\n","        correct = (pred == target).sum().item()\n","        total_correct += correct\n","        num_data += len(data)\n","\n","    #calculate loss and accuracy for one epoch\n","    average_loss = total_loss / len(train_loader)\n","    accuracy = 100. * total_correct / num_data\n","\n","    print(f'Train Epoch: {epoch}\\tAverage Loss: {average_loss:.6f}\\tAccuracy: {accuracy:.2f}%')\n","\n","    return accuracy, average_loss\n","\n","\n","# CutMix train function\n","def train_cutmix(model, criterion, train_loader, optimizer, epoch,alpha, device):\n","    model.train()\n","\n","    #for loss and accuracy tracking\n","    total_loss = 0.0\n","    total_correct = 0\n","    num_data = 0\n","\n","    for data, target in train_loader:\n","        data, target = cutmix(data, target, alpha)\n","        data=data.to(device)\n","        target=target.to(device)\n","        model.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        pred = torch.max(output, 1)[1]\n","        correct = (pred == target).sum().item()\n","        total_correct += correct\n","        num_data += len(data)\n","\n","    #calculate loss and accuracy for one epoch\n","    average_loss = total_loss / len(train_loader)\n","    accuracy = 100. * total_correct / num_data\n","\n","    print(f'Train Epoch: {epoch}\\tAverage Loss: {average_loss:.6f}\\tAccuracy: {accuracy:.2f}%')\n","\n","    return accuracy, average_loss\n","\n","\n","# Validation Function\n","def val(model, criterion, val_loader, device):\n","    model.eval()\n","\n","    #for loss and accuracy tracking\n","    val_loss = 0\n","    correct = 0\n","    total_correct= 0\n","    num_data = 0\n","\n","    with torch.no_grad():\n","        for data, target in val_loader:\n","            data= data.to(device)\n","            target= target.to(device)\n","\n","            output = model(data)\n","            loss=criterion(output,target)\n","            val_loss += loss.item()\n","            pred = torch.max(output, 1)[1]\n","            correct = (pred == target).sum().item()\n","            total_correct += correct\n","            num_data += len(data)\n","\n","    average_loss = val_loss / len(val_loader)\n","    test_acc = 100. * total_correct / num_data\n","    print(f'Val Loss: {average_loss:.4f}, Val Accuracy: {test_acc:.2f}%')\n","\n","    return test_acc, average_loss\n","\n","def criterion():\n","    \"\"\"\n","    Create and return a CrossEntropyLoss criterion.\n","    \"\"\"\n","    criterion = nn.CrossEntropyLoss()\n","    return criterion"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":10.396038,"end_time":"2023-11-10T02:03:13.356650","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-10T02:03:02.960612","version":"2.4.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}